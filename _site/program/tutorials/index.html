<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.4 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Tutorial - ALTA 2023</title>
<meta name="description" content="ALTA 2023 Tutorials">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="ALTA 2023">
<meta property="og:title" content="Tutorial">
<meta property="og:url" content="http://localhost:4000/program/tutorials/">


  <meta property="og:description" content="ALTA 2023 Tutorials">





  <meta name="twitter:site" content="@altanlp">
  <meta name="twitter:title" content="Tutorial">
  <meta name="twitter:description" content="ALTA 2023 Tutorials">
  <meta name="twitter:url" content="http://localhost:4000/program/tutorials/">

  
    <meta name="twitter:card" content="summary">
    
  

  







  

  


<link rel="canonical" href="http://localhost:4000/program/tutorials/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "alta",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="_pages/home.md" type="application/atom+xml" rel="alternate" title="ALTA 2023 Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="57x57" href="/assets/images/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/assets/images/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/assets/images/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/assets/images/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/assets/images/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/assets/images/apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/assets/images/apple-touch-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/assets/images/apple-touch-icon-152x152.png">
<link rel="icon" type="image/png" href="/assets/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/assets/images/favicon-16x16.png" sizes="16x16">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#2b5797">
<meta name="msapplication-TileImage" content="/assets/images/mstile-144x144.png">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    
<style type="text/css">
.tmp-disable{
color:#CCCCCC
}
</style>

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logos/alta_2023_logo.png" alt=""></a>
        
        <a class="site-title" href="/"> </a>
        <ul class="visible-links"><li class="masthead__menu-item">
                <a href="/calls/main_conference_papers" >Calls</a>
              </li><li class="masthead__menu-item">
                <a href="/program/" >Programme</a>
              </li><li class="masthead__menu-item">
                <a href="/program/keynotes/" ><span style="color:#999999;">Keynotes</span></a>
              </li><li class="masthead__menu-item">
                <a href="/venue" >Venue</a>
              </li><li class="masthead__menu-item">
                <a href="/registration/" >Registration</a>
              </li><li class="masthead__menu-item">
                <a href="https://www.alta.asn.au/events/sharedtask2023/" ><span style="color:#999999;">Shared Task</span></a>
              </li><li class="masthead__menu-item">
                <a href="/organization" >Committees</a>
              </li><li class="masthead__menu-item">
                <a href="https://www.alta.asn.au/mentoring/index.html" ><span style="color:#999999;">Mentoring</span></a>
              </li><li class="masthead__menu-item">
                <a href="/sponsors/" >Sponsors</a>
              </li><li class="masthead__menu-item">
                <a href="/contact" >Contact</a>
              </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">Program Details</span>
        

        
        <ul>
          
            
            

            
            

            <li><a href="/program/" class="">Conference Program</a></li>
          
            
            

            
            

            <li><a href="/program/keynotes/" class="">Keynotes</a></li>
          
            
            

            
            

            <li><a href="/program/tutorials/" class="active">Tutorial</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>
    
  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Tutorial">
    <meta itemprop="description" content="ALTA 2023 Tutorials">
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Tutorial
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-cog"></i> On this page</h4></header>
              <ul class="toc__menu">
  <li><a href="#natural-language-processing-for-clinical-text">Natural Language Processing for Clinical Text</a>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#speakers">Speakers</a></li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <h1 id="natural-language-processing-for-clinical-text">Natural Language Processing for Clinical Text</h1>

<h2 id="abstract">Abstract</h2>
<p>Learning from real-world clinical data has great potential to promote the quality of care, improve the efficiency of healthcare systems, and support clinical research. As a large proportion of clinical information is recorded only in unstructured free-text format, applying NLP to process and understand the vast amount of clinical text generated in clinical encounters is essential. Meanwhile, clinical text is known to be messy, contain complicated terminologies requiring clinical expertise to understand and annotate, and is written in different clinical contexts with distinct purposes. All these factors together make clinical NLP research both promising and challenging. In this tutorial, we will discuss the characteristics of clinical text and provide an overview of the classical clinical NLP tools used to process it. We will also present a real-world example to show the effectiveness of different NLP methods in processing and understanding clinical text. Finally, we will discuss how language models can be applied to often scarce clinical datasets, including their strengths and limitations in the medical context.</p>

<h2 id="speakers">Speakers</h2>
<p>By Wendy Chapman, Mike Conway, Jinghui Liu, Vlada Rozova</p>

<p><strong>Wendy Chapman</strong> is Associate Dean of Digital Health and Informatics and the Director of the Centre for the Digital Transformation of Health in the University of Melbourne’s Faculty of Medicine, Dentistry, and Health Sciences.  Prof Chapman has an extensive track record in the development and validation of NLP tools and resources for a variety of clinical NLP applications, including negation detection, deidentification, and – more broadly – clinical information extraction.</p>

<p><strong>Mike Conway</strong> is a senior lecturer in digital health at the University of Melbourne’s School of Computing &amp; Information Systems and the Centre for Digital Transformation of Health.  His research interests are centred on the application of computational methods — particularly natural language processing — to public health research questions, with much of his research output focused on the broad areas of communicable diseases, mental health, and substance use.</p>

<p><strong>Jinghui Liu</strong> is a Postdoctoral Research Fellow at the Australian e-Health Research Centre, Commonwealth Scientific and Industrial Research Organisation (CSIRO). He is interested in studying and applying natural language processing and machine learning techniques to healthcare data and how they can contribute to realising the potential of digital health. He recently obtained his PhD degree from the University of Melbourne.</p>

<p><strong>Vlada Rozova</strong> is a Postdoctoral Research Fellow with the Centre for Digital Transformation of Health at the University of Melbourne. She is a data scientist and a machine learning practitioner passionate about developing automated systems that can facilitate clinical decision-making. Vlada works with stakeholders of diverse backgrounds to build solutions that address user needs and enjoys seeing the development and implementation of tools from start to end.</p>

<!-- trailer -->
<!-- The following speakers have graciously agreed to give a tutorial at ALTA 2023:
- Mike Conway 
- Vlada Rozova 
- Wendy Chapman

More details about the tutorial for ALTA 2023 will be announced soon. -->

<!-- ## Tutorial Schedule

## T1 (Wednesday 7 Dec, Morning & Afternoon): Meaning Representations for Natural Languages: Design, Models and Applications
{: #t1}

Jeffrey Flanigan, Ishan Jindal, Yunyao Li, Tim O'Gorman, Martha Palmer

## T2 (Wednesday 7 Dec, Morning): Arabic Natural Language Processing
{: #t2}

Nizar Habash

## T3 (Wednesday 7 Dec, Afternoon): Emergent Language-Based Coordination In Deep Multi-Agent Systems  
{: #t3}

Marco Baroni, Roberto Dessi, Angeliki Lazaridou

## T4 (Thursday 8 December, Morning): Tutorial on Causal Inference for Natural Language Processing
{: #t4}

Zhijing Jin, Amir Feder, Kun Zhang

## T5 (Thursday 8 December, Morning): Modular and Parameter-Efficient Fine-Tuning for NLP Models
{: #t5}

Sebastian Ruder, Jonas Pfeiffer, Ivan Vulic

## T6 Thursday 8 December, Afternoon): Non-Autoregressive Models for Fast Sequence Generation  
{: #t6}

Yang Feng, Chenze Shao -->

<!-- ## T1 (Morning, 8-12): Pretrained Transformers for Text Ranking: BERT and Beyond
{: #t1}

<a href="mailto:ayates@mpi-inf.mpg.de">Andrew Yates</a>,
 <a href="mailto:rodrigo.nogueira@uwaterloo.ca">Rodrigo Nogueira</a> and
 <a href="mailto:jimmylin@uwaterloo.ca">Jimmy Lin</a>

**LIVE**

The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This tutorial, based on a forthcoming book, provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. We provide a synthesis of existing work as a single point of entry for both researchers and practitioners. Our coverage is grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. Two themes pervade our treatment: techniques for handling long documents, and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency). Although transformer architectures and pretraining techniques are recent innovations, many aspects of their application are well understood. Nevertheless, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, we also attempt to prognosticate the future.


## T2 (Morning, 8-12): Fine-grained Interpretation and Causation Analysis in Deep NLP Models
{: #t2}

<a href="mailto:hsajjad@hbku.edu.qa">Hassan Sajjad</a>,
 <a href="mailto:narine@fb.com">Narine Kokhlikyan</a>,
 <a href="mailto:faimaduddin@hbku.edu.qa">Fahim Dalvi</a> and
 <a href="mailto:ndurrani@hbku.edu.qa">Nadir Durrani</a>

**Pre-recorded video played during session w/Q&A**

Deep neural networks have constantly pushed the state-of-the-art performance in natural language processing and are considered as the de-facto modeling approach in solving complex NLP tasks such as machine translation, summarization and question-answering. Despite the proven efficacy of deep neural networks at-large, their opaqueness is a major cause of concern.

In this tutorial, we will present research work on interpreting fine-grained components of a neural network model from two perspectives, i) intrinsic analysis, and  ii) causation analysis. The former is a class of methods to analyze neurons with respect to a desired language concept or a task. The latter studies the role of neurons and input features in explaining the decisions made by the model. We will also discuss how interpretation methods and causation analysis can connect towards better interpretability of model prediction. Finally, we will walk you through various toolkits that facilitate fine-grained interpretation and causation analysis of neural models.


## T3 (Morning, 8-12): Deep Learning on Graphs for Natural Language Processing
{: #t3}

<a href="mailto:lwu@email.wm.edu">Lingfei Wu</a>,
 <a href="mailto:hugochan2013@gmail.com">Yu Chen</a>,
 <a href="mailto:hengji@illinois.edu">Heng Ji</a> and
 <a href="mailto:yunyaoli@us.ibm.com">Yunyao Li</a>

**LIVE**

Due to its great power in modeling non-Euclidean data like graphs or manifolds, deep learning on graph techniques (i.e., Graph Neural Networks (GNNs)) have opened a new door to solving challenging graph-related NLP problems. There has seen a surge of interests in applying deep learning on graph techniques to NLP, and has achieved considerable success in many NLP tasks, ranging from classification tasks like sentence classification, semantic role labeling and relation extraction, to generation tasks like machine translation, question generation and summarization. Despite these successes, deep learning on graphs for NLP still face many challenges, including automatically transforming original text sequence data into highly graph-structured data, and effectively modeling complex data that involves mapping between graph-based inputs and other highly structured output data such as sequences, trees, and graph data with multi-types in both nodes and edges.

This tutorial will cover relevant and interesting topics on applying deep learning on graph techniques to NLP, including automatic graph construction for NLP, graph representation learning for NLP, advanced GNN based models (e.g., graph2seq, graph2tree, and graph2graph) for NLP, and the applications of GNNs in various NLP tasks (e.g., machine translation, natural language generation, information extraction and semantic parsing). In addition, hands-on demonstration sessions will be included to help the audience gain practical experience on applying GNNs to solve challenging NLP problems using our recently developed open source library -- Graph4NLP, the first library for researchers and practitioners for easy use of GNNs for various NLP tasks.


## T4 (Morning, 8-12): A Tutorial on Evaluation Metrics used in Natural Language Generation
{: #t4}

<a href="mailto:miteshk@cse.iitm.ac.in">Mitesh M. Khapra</a> and
 <a href="mailto:ananya@cse.iitm.ac.in">Ananya B. Sai</a>

**Pre-recorded video played during session w/Q&A**

The advent of Deep Learning and the availability of large scale datasets has accelerated research on Natural Language Generation with a focus on newer tasks and better models. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation metrics. Especially in the last few years, there has been an increasing focus on evaluation metrics, with several criticisms of existing metrics and proposals for several new metrics.

This tutorial presents the evolution of automatic evaluation metrics to their current state along with the emerging trends in this field by specifically addressing the following questions:
- What makes NLG evaluation challenging?
- Why do we need automatic evaluation metrics?
- What are the existing automatic evaluation metrics and how can they be organised in a coherent taxonomy?
- What are the criticisms and shortcomings of existing metrics?
- What are the possible future directions of research?


## T5 (Morning, 8-12): Beyond Paragraphs: NLP for Long Sequences
{: #t5}

<a href="mailto:beltagy@allenai.org">Iz Beltagy</a>,
 <a href="mailto:armanc@allenai.org">Arman Cohan</a>,
 <a href="mailto:hannaneh@washington.edu">Hannaneh Hajishirzi</a> and
 <a href="mailto:matthewp@allenai.org">Matthew E. Peters</a>
 <a href="mailto:sewon@cs.washington.edu">Sewon Min</a>

**Pre-recorded video played during session w/Q&A**

A significant subset of natural language data includes documents that span thousands of tokens. The ability to process such long sequences is critical for many NLP tasks including document classification, summarization, multi-hop, and open domain question answering, and document-level or multi-document relationship extraction and coreference resolution. Yet, scaling state-of-the-art models to long sequences is challenging as many models are designed for shorter sequences. One notable example is Transformer models that have quadratic computational cost in the sequence length, making them prohibitively expensive for long sequence tasks. This is reflected in many widely-used models such as RoBERTa and BERT where the sequence length is limited to only 512 tokens. In this tutorial, we will bring interested NLP researchers up to speed about the recent and ongoing techniques for document-level representation learning. Additionally, we will discuss new research opportunities to address existing challenges in this domain. We will first provide an overview of established long sequence NLP techniques, including hierarchical, graph-based, and retrieval-based methods. We will then focus on the recent long-sequence transformer methods, how they compare to each other, and how they can be applied to NLP tasks (see Tay et al. (2020) for a recent survey). We will also discuss various memory-saving methods that are key to processing long sequences. Throughout the tutorial, we will use classification, question answering, and information extraction as motivating tasks.  We will also have a hands-on coding exercise focused on summarization.


## T6 (Afternoon, 4-8): Crowdsourcing Natural Language Data at Scale: A Hands-On Tutorial
{: #t6}

<a href="mailto:adrutsa@yandex-team.ru">Alexey Drutsa</a>,
 <a href="mailto:dustalov@yandex-team.ru">Dmitry Ustalov</a>,
 <a href="mailto:valya17@yandex-team.ru">Valentina Fedorova</a>,
 <a href="mailto:omegorskaya@yandex-team.ru">Olga Megorskaya</a> and
 <a href="mailto:dbaidakova@yandex-team.ru">Daria Baidakova</a>

**LIVE**

In this introductory tutorial, we present a portion of our six-year-long unique industry experience in efficient natural language data annotation via Crowdsourcing. We will make an introduction to data labeling via public crowdsourcing marketplaces, and will present the key components of efficient label collection that includes task design and decomposition, quality control, and annotator selection. This will be followed by a practical session, where participants address a real-world language resource production task, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session. We will present useful mathematical foundations, quality control techniques and tricks, and provide the attendees with an opportunity to discuss their own annotation ideas.
 -->

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/altanlp" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/ruixing76/alta2023" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.alta.asn.au/" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> ALTA</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="_pages/home.md"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 alta. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>













  </body>
</html>
